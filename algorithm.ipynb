{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "#load training dataset from file into pandas dataframe\n",
    "rows_list = []\n",
    "with open('mediaeval-2015-trainingset.txt', 'r', encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        entry = [l.strip() for l in line.split('\\t')]\n",
    "        rows_list.append(entry)\n",
    "training = pd.DataFrame(rows_list[1:], columns = rows_list[0])\n",
    "\n",
    "#import test data from file, turn into DataFrame\n",
    "trows = []\n",
    "with open('mediaeval-2015-testset.txt', 'r', encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        entry = [l.strip() for l in line.split('\\t')]\n",
    "        trows.append(entry)\n",
    "testing = pd.DataFrame(trows[1:], columns = trows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve classifier performance, the training dataset must go through some preprocessing.\n",
    "\n",
    "A regular expression is used to remove URLs and remove punctuation marks from the tweet text, which is then converted to lower case.\n",
    "\n",
    "We also convert all 'humor' labels to 'fake' labels since humor tweets are to be considered fake when evaluating the dataset, and it was found that better results were achieved when the two classes were merged in training instead of keeping them separate in training, and then converting 'humor' predictions into 'fake' predictions in testing.\n",
    "\n",
    "We also apply the same tweet preprocessing steps to the tweets in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processes a tweet's text to remove certain punctuation, remove urls and trim to lower case\n",
    "def processTweet(tweet):\n",
    "    return re.sub('https?:\\/\\/t.co\\/[0-9a-zA-Z]*|\\\\n|&amp;|&gt;|&lt;|#|\\\"|<|>|\\(|\\)|\\'|\\*|\\-|_|=|\\+|%', '', tweet).lower()\n",
    "\n",
    "#process each tweet, replace tweet series with new list of processed tweets\n",
    "processed_tweets = []\n",
    "for tweet in training['tweetText']:\n",
    "    processed_tweets.append(processTweet(tweet))\n",
    "training['tweetText'] = processed_tweets  \n",
    "\n",
    "#change humour label to fake label in training\n",
    "new_labels = []\n",
    "for label in training['label']:\n",
    "    if label == 'humor':\n",
    "        new_labels.append('fake')\n",
    "    else:\n",
    "        new_labels.append(label)\n",
    "training['label'] = new_labels\n",
    "\n",
    "#preprocess test set in the same way\n",
    "processed_test_tweets = []\n",
    "for tweet in testing['tweetText']:\n",
    "    processed_test_tweets.append(processTweet(tweet))\n",
    "testing['tweetText'] = processed_test_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create algorithm pipeline, make predictions and evaluate classifier performance\n",
    "\n",
    "We build our classifier in the form of a Pipeline to make the process of building and training the classifier more concise.\n",
    "First, a CountVectorizer tokenises tweets, converts them into n-grams and adds them into a bag of words.\n",
    "Next, a TfidfTransformer performs Text Frequency Inverse Document Frequency (TF-IDF) on the n-grams in each tweet, which identifies how frequently each phrase appears in a compared to the rest of the bag of words.\n",
    "Finally, a MultinomialNB is trained on the tf-idf feature vectors of each tweet.\n",
    "\n",
    "After training the classifier, we pass in the preprocessed test dataset and obtain class predictions for each tweet. We then evaluate the classifier by comparing the predictions to the ground truths provided, and calculate an F1 score for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8614123247818037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.87      0.93      0.90      2564\n",
      "        real       0.84      0.71      0.77      1217\n",
      "\n",
      "    accuracy                           0.86      3781\n",
      "   macro avg       0.85      0.82      0.83      3781\n",
      "weighted avg       0.86      0.86      0.86      3781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create algorithm pipeline\n",
    "#first tokenise text using CountVectorizer, removal of stopwords and creation of ngrams occurs here\n",
    "#then calculate tf-idf of tokens\n",
    "#then train MultinomialNB classifier using tf-idf\n",
    "#this pipeline contains the best parameters found for chosen algorithm design, use this to check f1 scores\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "tweet_clf = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,7), max_features=17500)),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB())\n",
    "])\n",
    "tweet_clf.fit(training['tweetText'], training['label'])\n",
    "predicted = tweet_clf.predict(testing['tweetText'])\n",
    "\n",
    "#get metrics\n",
    "#print(metrics.f1_score(testing['label'], predicted, average='micro', labels=labels))\n",
    "print(metrics.f1_score(testing['label'], predicted, average='micro'))\n",
    "print(metrics.classification_report(testing['label'], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Parameter tuning\n",
    "\n",
    "Different combinations of n-gram ranges and max features lead to different F1 scores. Because of this, we want to find the combination of the two which yields the best score.\n",
    "\n",
    "We do this here by iteration through different classifier configurations and obtaining their F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,3), 10000: 0.7751917482147579\n",
      "(1,3), 10500: 0.8328484527902671\n",
      "(1,3), 11000: 0.7741338270298864\n",
      "(1,3), 11500: 0.8328484527902671\n",
      "(1,3), 12000: 0.7746627876223221\n",
      "(1,3), 12500: 0.775456228510976\n",
      "(1,3), 13000: 0.7751917482147579\n",
      "(1,3), 13500: 0.7746627876223221\n",
      "(1,3), 14000: 0.775456228510976\n",
      "(1,3), 14500: 0.7667283787357841\n",
      "(1,3), 15000: 0.7667283787357841\n",
      "(1,4), 10000: 0.6612007405448294\n",
      "(1,4), 10500: 0.7889447236180903\n",
      "(1,4), 11000: 0.7900026448029621\n",
      "(1,4), 11500: 0.7929119280613595\n",
      "(1,4), 12000: 0.7913250462840519\n",
      "(1,4), 12500: 0.7915895265802699\n",
      "(1,4), 13000: 0.7923829674689234\n",
      "(1,4), 13500: 0.7913250462840519\n",
      "(1,4), 14000: 0.7918540068764877\n",
      "(1,4), 14500: 0.7931764083575774\n",
      "(1,4), 15000: 0.7929119280613595\n"
     ]
    }
   ],
   "source": [
    "#evaluate different configurations of algorithm design\n",
    "#adjust list values for different intervals\n",
    "feature_intervals = [2000, 3000, 5000, 7500, 10000, 15000, 20000, 30000, 40000]\n",
    "intervals = [10000,10500,11000,11500,12000,12500,13000,13500,14000,14500,15000]\n",
    "#adjust ranges to try different n-gram ranges\n",
    "for min_ngrams in range(1,2):\n",
    "    for max_ngrams in range(3,5):\n",
    "        for features in intervals:\n",
    "            config_clf = Pipeline([\n",
    "             ('vect', CountVectorizer(ngram_range=(min_ngrams,max_ngrams), max_features = features)),\n",
    "             ('tfidf', TfidfTransformer()),\n",
    "             ('clf', MultinomialNB()),])\n",
    "            config_clf.fit(training['tweetText'], training['label'])\n",
    "            config_predict = config_clf.predict(testing['tweetText'])\n",
    "            config_score = metrics.f1_score(testing['label'], config_predict, average='micro')\n",
    "            print(\"(\"+str(min_ngrams)+\",\"+str(max_ngrams)+\"), \"+str(features)+\": \"+str(config_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
